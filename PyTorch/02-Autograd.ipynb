{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mZLmaW4x4Qai"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Calculating gradient using autograd"
      ],
      "metadata": {
        "id": "jFsg5yAG8LkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a tensor with gradient tracking\n",
        "x = torch.tensor(3.0, requires_grad = True)\n",
        "\n",
        "# Defining a function\n",
        "y = x**2\n",
        "z = torch.sin(y)\n",
        "\n",
        "# Computing gradient of root node(z) w.r.t leaf node(x)\n",
        "z.backward()\n",
        "\n",
        "# Calculated derivative of z w.r.t x, gets sotred in x.grad\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV0ykCH-8cGJ",
        "outputId": "0a04b3f0-4bdd-40de-de33-20ca468d0014"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-5.4668)\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1256227384.py:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(y.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Calculating gradient for intermediate nodes (like y, in above example)\n"
      ],
      "metadata": {
        "id": "NIm_PWFCwLAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function again\n",
        "y = x**2\n",
        "y.retain_grad()\n",
        "z = torch.sin(y)\n",
        "\n",
        "z.backward(retain_graph = True)   # we have to put \"retain_graph = True\", when we are calling the .backward() operation again on the same graph\n",
        "\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "metadata": {
        "id": "hxNi92kd-fvg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de975081-2ed3-48cf-b5d7-bba36ea7d9ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-10.9336)\n",
            "tensor(-0.9111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1) Gradients Accumulate by Default in PyTorch\n",
        "\n",
        "In PyTorch, the `.grad` attribute does **not** reset automatically after each backward pass. Instead, gradients are **accumulated** (added up) by default.  \n",
        "\n",
        "This behavior is very useful during training (e.g., summing gradients over multiple mini-batches), but it can cause confusion during experiments if you forget to clear them.  \n",
        "ðŸ‘‰ So, when you compute derivatives multiple times, the gradients get **accumulated**.  \n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Example Walkthrough  \n",
        "\n",
        "**First backward pass:**  \n",
        "\n",
        "$$\n",
        "\\frac{dz}{dx} = \\cos(x^2) \\cdot 2x \\approx -5.4668\n",
        "$$  \n",
        "\n",
        "**Second backward pass (same function again):**  \n",
        "\n",
        "PyTorch adds the new gradient to the old one:  \n",
        "\n",
        "$$\n",
        "-5.4668 + (-5.4668) = -10.9336\n",
        "$$  \n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ How to Fix This  \n",
        "Always clear gradients **before** calling `.backward()` again if you want fresh values:  \n",
        "\n",
        "```\n",
        "x.grad = None      # or x.grad.zero_()\n",
        "\n",
        "y.grad = None\n",
        "\n",
        "z.backward()\n",
        "```"
      ],
      "metadata": {
        "id": "uWFf3jty4hGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function again\n",
        "y = x**2\n",
        "y.retain_grad()\n",
        "z = torch.sin(y)\n",
        "\n",
        "# Clearing the gradients\n",
        "x.grad = None\n",
        "y.grad = None\n",
        "\n",
        "z.backward(retain_graph = True)   # we have to put \"retain_graph = True\", when we are calling the .backward() operation again on the same graph\n",
        "\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUDmUEnq5OM3",
        "outputId": "166c33bd-4b7a-4054-bb0e-54c47d51ad86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-5.4668)\n",
            "tensor(-0.9111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2) Calculating intermediate gradients on completely new tensor\n"
      ],
      "metadata": {
        "id": "FPg6BOGF9NFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating same tensor with different name\n",
        "a = torch.tensor(3.0, requires_grad = True)                         # a --> x\n",
        "\n",
        "# Define function\n",
        "b = a**2                       # b --> y\n",
        "b.retain_grad()\n",
        "c = torch.sin(b)               # c --> z\n",
        "\n",
        "# Call backward pass; calculates and store the gradients\n",
        "c.backward()                                                  # Here we don't need to pass \"retain_graph = True\"\n",
        "\n",
        "# Print the calculated derivatives/gradients\n",
        "print(a.grad)                                     # a.grad --> x.grad\n",
        "print(b.grad)                                     # b.grad --> y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmKDjbUVsifJ",
        "outputId": "d313667b-b36e-4024-b300-6485a6f08c1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-5.4668)\n",
            "tensor(-0.9111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Disabling Gradient Calculation in PyTorch\n",
        "\n",
        "By default, PyTorch tracks operations on tensors with `requires_grad=True` to build a computation graph for backpropagation.  \n",
        "Sometimes, you donâ€™t want gradients (e.g., during inference, or when freezing parts of a model).  \n",
        "\n",
        "There are **three main ways** to disable gradient calculation:\n",
        "\n"
      ],
      "metadata": {
        "id": "JF2GIiMev-s5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1)  Option 1: `requires_grad_(False)`\n",
        "- Permanently tells PyTorch to **stop tracking gradients** for this tensor.  \n",
        "- The tensor becomes a \"leaf\" without gradient history.  "
      ],
      "metadata": {
        "id": "0h03OpBExbSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor(3.0, requires_grad = True)\n",
        "\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVKrXYDxynr1",
        "outputId": "53d39b40-130b-4649-fa9e-f67ce58ea8d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabling gradients\n",
        "\n",
        "a.requires_grad_(False)                # now gradients won't be tracked\n",
        "\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRuQNIdHyUju",
        "outputId": "8247ee6a-ef76-4459-a257-46dd29d17be8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2) Option 2: `.detach()`\n",
        "- Returns a **new tensor** that shares the same data as the original but **without gradients**.  \n",
        "- The original tensor is unaffected.  \n",
        "- Useful for intermediate computations where you want to \"cut off\" the gradient flow.\n"
      ],
      "metadata": {
        "id": "VQbqNdbVxTme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor(3.0, requires_grad = True)\n",
        "\n",
        "b = a.detach()                        # b is a tensor without gradient tracking\n",
        "\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyc6uhNfywew",
        "outputId": "1baff991-6d6d-43a1-faa9-abf7299aa01b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(3.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3) Option 3: `torch.no_grad()`\n",
        "\n",
        "- A **context manager** that disables gradient tracking for everything inside its block.  \n",
        "- Commonly used during **inference** or **evaluation** (to save memory and computation).  "
      ],
      "metadata": {
        "id": "0nuZs2VYyxEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor(3.0, requires_grad = True)\n",
        "\n",
        "with torch.no_grad():                      # No gradient tracking under this block\n",
        "    b = a**2\n",
        "\n",
        "c = torch.sin(a)\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7SOKN2Ay1jt",
        "outputId": "11557b06-4527-46a7-f82b-b8d99ac51487"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(9.)\n",
            "tensor(0.1411, grad_fn=<SinBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrpaXxMc08-5"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}