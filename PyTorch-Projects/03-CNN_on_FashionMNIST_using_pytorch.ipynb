{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uwZvXSNNZbKj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seting random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v50W4_QB60S8",
        "outputId": "d399aa4f-1406-47c4-fff1-72b3d4476eab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bae62087d50>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Available device: ', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UXbxgZK7CnY",
        "outputId": "f6a32854-1bd3-4ac8-8a80-7baa3e02b1f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Loading Fashion MNIST dataset\n",
        "\n",
        "#### ABOUT THE FASHION_MNIST DATASET:\n",
        "\n",
        "The Fashion-MNIST is a dataset of Zalando's article images. The Fashion-MNIST dataset is a popular dataset used for benchmarking machine learning models, particularly in computer vision. It is designed as a drop-in replacement for the original MNIST dataset but contains images of clothing items instead of handwritten digits.\n",
        "\n",
        "#### **Overview of the Fashion-MNIST Dataset:**\n",
        "- **Size:** 70,000 grayscale images  \n",
        "- **Training set:** 60,000 images  \n",
        "- **Test set:** 10,000 images  \n",
        "- **Image dimensions:** 28x28 pixels  \n",
        "- **Number of classes:** 10 (each representing a type of clothing)  \n",
        "\n",
        "#### **Classes (Labels) in Fashion-MNIST**\n",
        "Each image belongs to one of the following 10 classes:\n",
        "\n",
        "| Label | Class Name     |\n",
        "|-------|--------------|\n",
        "| 0     | T-shirt/top  |\n",
        "| 1     | Trouser      |\n",
        "| 2     | Pullover     |\n",
        "| 3     | Dress        |\n",
        "| 4     | Coat         |\n",
        "| 5     | Sandal       |\n",
        "| 6     | Shirt        |\n",
        "| 7     | Sneaker      |\n",
        "| 8     | Bag          |\n",
        "| 9     | Ankle boot   |\n",
        "\n",
        "#### **Key Features:**\n",
        "- The dataset is more challenging than MNIST because the images have more complex patterns.  \n",
        "- It is intended to be used as a benchmark for machine learning and deep learning models.  \n",
        "- Each pixel in the images has values ranging from **0 to 255** (grayscale intensity).  \n",
        "- Unlike color images (RGB), these images have only **one channel**.  \n",
        "- The images are already preprocessed, so they are in **NumPy array format**.  \n"
      ],
      "metadata": {
        "id": "OQlgf6WMNYgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsocKQdW7fJj",
        "outputId": "549fc088-eadd-45f6-e31b-b99c9419dad7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1x7vilENjzG",
        "outputId": "9c934916-aac2-4917-e441-a879e404e04f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
        "\n",
        "# 1st image\n",
        "axes[0].imshow(x_train[31])\n",
        "axes[0].set_title('Label: '+ str(y_train[31]))\n",
        "\n",
        "# 2nd image in grayscale\n",
        "axes[1].imshow(x_train[13], cmap = 'gray')\n",
        "axes[1].set_title('Label: '+ str(y_train[13]))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "KUQC3MXiO9iH",
        "outputId": "6091317e-fd5c-4f67-82a7-c6f3f1b121a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Creating Dataset class"
      ],
      "metadata": {
        "id": "lUV6leEAwO1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, label, transform = None):\n",
        "\n",
        "    # Loading features and label from the dataset\n",
        "    self.features = features\n",
        "    self.label = label\n",
        "\n",
        "    self.transform = transform      # Defining transformation, if a transformation function is passed by user while making the object of this class\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    # Getting features and label of a particular row\n",
        "    x = self.features[idx]\n",
        "    y = self.label[idx]\n",
        "\n",
        "    if self.transform:\n",
        "      x = self.transform(x)                               # We get x as tensor after applying the transformation\n",
        "\n",
        "    x = x.reshape(1, 28, 28).float()                      # Reshape to (channels, height, width) and convert to float\n",
        "\n",
        "    y = torch.tensor(y, dtype = torch.long)               # Converting y to tensor too and setting dtype to long for CrossEntropyLoss\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "afkEefCazELQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Creating transformation function"
      ],
      "metadata": {
        "id": "7hzf5qm41nN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformation(features):\n",
        "\n",
        "  features = features/255.0                   # Scaling the data\n",
        "  features = torch.tensor(features)           # Converting to tensor\n",
        "  return features"
      ],
      "metadata": {
        "id": "HdZGuDFHQ2Y7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Creating Dataset & DataLoader object"
      ],
      "metadata": {
        "id": "UPbmMxiw18Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyDataset(x_train, y_train, transform = transformation)\n",
        "test_dataset = MyDataset(x_test, y_test, transform = transformation)"
      ],
      "metadata": {
        "id": "W0FScxYY1dqC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True, pin_memory = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False, pin_memory = True)"
      ],
      "metadata": {
        "id": "Tl0RSy9g2adI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Building CNN Architecture"
      ],
      "metadata": {
        "id": "ywvandqo3Cuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNmodel(nn.Module):\n",
        "\n",
        "  def __init__(self, num_in_channels):\n",
        "    super(CNNmodel, self).__init__()\n",
        "\n",
        "    # Creating convolution layers\n",
        "    self.convlayers = nn.Sequential(\n",
        "\n",
        "        nn.Conv2d(in_channels = num_in_channels, out_channels = 32, kernel_size = 3, padding = 'same'),    # in_channels: Number of channels in input (i.e grayscale=1, RGB=3),\n",
        "                                                                                             # out_channels: number of filters/kernels to be applied in the convolutional layer.\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "\n",
        "        nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 'same'),    # Here the in_channels is 32, cause the input is a feature map (28,28,32)\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Creating FC layers\n",
        "    self.FClayers = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "\n",
        "        nn.Linear(in_features = 64*7*7, out_features = 128),      # 'in_features' is number of inputs comming to the layer\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p = 0.3),\n",
        "\n",
        "        nn.Linear(in_features = 128, out_features = 64),          # 'out_features' is number of neurons in this layer\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p = 0.3),\n",
        "\n",
        "        nn.Linear(in_features = 64, out_features = 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    y_pred = self.convlayers(x)\n",
        "    y_pred = self.FClayers(y_pred)\n",
        "    return y_pred\n",
        "\n",
        "  def loss_function(self, y_pred, y):\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "kfj69e7g3CBJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Training the model"
      ],
      "metadata": {
        "id": "kS7EQbIz2lQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making object of the model\n",
        "model = CNNmodel(num_in_channels = 1)                 # All images are grayscale in dataset, so channels = 1\n",
        "print(model)\n",
        "\n",
        "# Creating optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4Wan2552Jce",
        "outputId": "39881aee-13f5-46e2-9390-8cd770290360"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNmodel(\n",
            "  (convlayers): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (5): ReLU()\n",
            "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (FClayers): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.3, inplace=False)\n",
            "    (7): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving the model to GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "iNV9MIKi381I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "\n",
        "for epoch in range(25):\n",
        "\n",
        "  total_epoch_loss = 0.0                                                    # Initialize total_epoch_loss for each epoch\n",
        "\n",
        "  for batch_features, batch_label in train_loader:\n",
        "\n",
        "    # move data to gpu\n",
        "    batch_features, batch_label = batch_features.to(device), batch_label.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    y_pred = model.forward(batch_features)\n",
        "\n",
        "    # loss calculation\n",
        "    loss = model.loss_function(y_pred, batch_label)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Reset the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_epoch_loss = total_epoch_loss + loss.item()\n",
        "\n",
        "  avg_loss = total_epoch_loss/len(train_loader)\n",
        "  print(f'Epoch: {epoch + 1} , Loss: {avg_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGUJDqm323j5",
        "outputId": "ce53994a-45c7-4287-9236-44a55a370786"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 , Loss: 0.4230326317369938\n",
            "Epoch: 2 , Loss: 0.2894171301474174\n",
            "Epoch: 3 , Loss: 0.2484630409821868\n",
            "Epoch: 4 , Loss: 0.22439276268184186\n",
            "Epoch: 5 , Loss: 0.2064360776702563\n",
            "Epoch: 6 , Loss: 0.18917190721084673\n",
            "Epoch: 7 , Loss: 0.1751848121235768\n",
            "Epoch: 8 , Loss: 0.16188538354256501\n",
            "Epoch: 9 , Loss: 0.15410368787224094\n",
            "Epoch: 10 , Loss: 0.1413416060609122\n",
            "Epoch: 11 , Loss: 0.1317558969175443\n",
            "Epoch: 12 , Loss: 0.12381397704531749\n",
            "Epoch: 13 , Loss: 0.11550566770940399\n",
            "Epoch: 14 , Loss: 0.109162228036051\n",
            "Epoch: 15 , Loss: 0.11089382017736013\n",
            "Epoch: 16 , Loss: 0.10231695378395574\n",
            "Epoch: 17 , Loss: 0.09472894479517514\n",
            "Epoch: 18 , Loss: 0.0944117819094332\n",
            "Epoch: 19 , Loss: 0.08832814564182967\n",
            "Epoch: 20 , Loss: 0.08284111544643528\n",
            "Epoch: 21 , Loss: 0.07818995015406788\n",
            "Epoch: 22 , Loss: 0.08167998822850447\n",
            "Epoch: 23 , Loss: 0.07586038278145134\n",
            "Epoch: 24 , Loss: 0.07245505283665843\n",
            "Epoch: 25 , Loss: 0.06966361249735734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Model Evaluation"
      ],
      "metadata": {
        "id": "nNmAwLS3AbCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8CxfnZt4v6q",
        "outputId": "df578d39-ea67-47fe-8100-64a56ed6a498"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNmodel(\n",
              "  (convlayers): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (5): ReLU()\n",
              "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (FClayers): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Dropout(p=0.3, inplace=False)\n",
              "    (7): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of model on test data\n",
        "\n",
        "correct_pred = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for batch_features, batch_label in test_loader:\n",
        "\n",
        "    # Move data on GPU\n",
        "    batch_features, batch_label = batch_features.to(device), batch_label.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    y_test_pred = model.forward(batch_features)\n",
        "\n",
        "    # Get predicted class index\n",
        "    _, predicted = torch.max(y_test_pred, 1)\n",
        "\n",
        "    # Check if correctly predicted\n",
        "    correct_pred += (predicted == batch_label).sum().item()\n",
        "\n",
        "    # Total samples\n",
        "    total_samples += batch_label.shape[0]\n",
        "\n",
        "\n",
        "# Calculating Accuracy\n",
        "acc = correct_pred/total_samples\n",
        "print(f'Accuracy on test data: {acc*100}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbOUWIZEAiB7",
        "outputId": "7065c7f3-1856-4ba8-999d-4dd2684d3179"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 91.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MbDNeoqZBtAk"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}